{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24f7094-950c-428c-9b64-5c6d8d4a9549",
   "metadata": {},
   "source": [
    "# Geoembedding-based Land Cover Modelling Pipeline\n",
    "\n",
    "**Author:** Pablo Timoner  \n",
    "**Date:** 2026-01-20\n",
    "\n",
    "This notebook implements a **land cover classification workflow** using reference data (ESA World Cover or CSV tables) and raster-based geoembeddings. The main steps are:\n",
    "\n",
    "## 1. Reference Data Preparation\n",
    "- Load reference data: either a **CSV table** or an **ESA World Cover raster** (`is_esa=True`).\n",
    "- If ESA raster:\n",
    "  - Mask `nodata` pixels.\n",
    "  - Compute class distribution and proportionally allocate sample points per class.\n",
    "  - Perform stratified random sampling of pixel locations.\n",
    "  - Convert sampled pixels to geographic coordinates (`lon/lat`).\n",
    "  - Create a DataFrame for further processing\n",
    "- If the embedding raster CRS differs from WGS84:\n",
    "  - Transform coordinates to match embedding CRS.\n",
    "  - Add `x_<EPSG>` / `y_<EPSG>` columns for modelling.\n",
    "- Reclassify land cover classes using a mapping table.\n",
    "\n",
    "## 2. Sampling & Feature Extraction\n",
    "- Keep only points that fall within the embedding raster extent.\n",
    "- **Extract embedding values** at sampled point locations.\n",
    "\n",
    "## 3. Training / Test Split\n",
    "- Limit total number of samples (`n_sample`) to reduce computational cost.\n",
    "- Remove classes with extremely low representation (<1% of samples).\n",
    "- Split sampled points into **training and testing sets**.\n",
    "- Stratify split by land cover class to preserve class proportions.\n",
    "\n",
    "## 4. Random Forest Model\n",
    "- Initialize Random Forest classifier.\n",
    "- Stage 1 hyperparameter tuning(tree-level):\n",
    "  - `max_features`, `max_depth`, `min_samples_leaf`\n",
    "- Stage 2 hyperparameter tuning (forest-level):\n",
    "  - `n_estimators`, `min_samples_split`, `class_weight`\n",
    "- Select best hyperparameters and fit **final model** on all samples.\n",
    "\n",
    "## 5. Raster Prediction\n",
    "- Predict land cover for large raster in **blocks** to save memory (`block_shape` can be adapted).\n",
    "- Steps per block:\n",
    "  1. Read block from embedding raster.\n",
    "  2. Flatten block to `(pixels x features)` format.\n",
    "  3. Apply trained Random Forest to predict class labels.\n",
    "  4. Reshape predictions to block shape and write to output raster.\n",
    "- Continue iteratively until the entire raster is processed.\n",
    "- Save final predicted raster as `.tif`.\n",
    "\n",
    "## Notes\n",
    "- Designed to handle **large rasters** and **multiple embedding bands** efficiently.\n",
    "- Fully reproducible with **stratified sampling**, **fixed random seed**, and saved hyperparameters.\n",
    "- Can handle any **EPSG CRS** for embeddings and reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from rasterio.transform import xy\n",
    "from pyproj import Transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abf380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference data (can be a CSV table or the path to a ESA World Cover raster)\n",
    "# If ESA World Cover must be EPSG:4326\n",
    "ref_path = \"/home/jupyterhome/jupyter-pablotimoner/Geoembeddings/Manaus/ESA_WorldCover_2021_manaus.tif\"\n",
    "\n",
    "# Is the reference data a raster of the ESA World Cover\n",
    "is_esa = True\n",
    "\n",
    "# If is_esa = True, how many pixels to potentially be considered for modelling (calibration & validation)\n",
    "n_esa_sample = 10000\n",
    "\n",
    "# If is_esa = False names of columns in the reference data table where we have the coordinates, and where we have the land cover class\n",
    "X_col = 'lon'\n",
    "Y_col = 'lat'\n",
    "LC_col = 'AS_17'\n",
    "\n",
    "# Geoembedding path\n",
    "embedding_path = \"/home/jupyterhome/jupyter-pablotimoner/Geoembeddings/Manaus/EE_exports/embedding_2020_merged.tif\"\n",
    "\n",
    "# Land cover class mapping table (requires at least two columns: 'code' for original code and 'value' for new code)\n",
    "class_table_path = \"/home/jupyterhome/jupyter-pablotimoner/Geoembeddings/LC_classes/CLASSES_ESA.csv\"\n",
    "\n",
    "# Total number of observations used for modelling (limited to balance computational cost)\n",
    "# When is_esa=True and n_esa_sample is smaller than n_sample,\n",
    "# n_sample will be reduced automatically to match n_esa_sample\n",
    "n_sample = 5000\n",
    "\n",
    "# Number of cores for parallel computing (RAM is usually the limiting factor ; keep the number of cores low)\n",
    "ncore = 2\n",
    "\n",
    "# Output folder path\n",
    "output_folder = \"/home/jupyterhome/jupyter-pablotimoner/Geoembeddings/Manaus/results/AlphaEarth/test\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82eacdf-208f-41fd-aeee-c475c24aafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names for intermediate output files (without extension)\n",
    "sample_filename = 'samples'\n",
    "final_df_filename = 'df_final'\n",
    "best_params_i_filename = 'best_params1'\n",
    "best_params_ii_filename = 'best_params2'\n",
    "report_filename = 'report'\n",
    "best_model_filename = 'best_model'\n",
    "prediction_filename = 'lc_prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bfc4d-bb3f-41fa-84c1-a57b8998a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read or prepare the reference data\n",
    "if not is_esa:\n",
    "    df = pd.read_csv(ref_path)\n",
    "else:\n",
    "    print(\"Preparing the reference data using ESA World Cover...\")\n",
    "    with rasterio.open(ref_path) as src:\n",
    "        data = src.read(1)\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        nodata = src.nodata\n",
    "    \n",
    "    # Mask nodata\n",
    "    if nodata is not None:\n",
    "        mask = data != nodata\n",
    "        data_valid = data[mask]\n",
    "    else:\n",
    "        data_valid = data\n",
    "    \n",
    "    # # Compute class distribution from valid pixels\n",
    "    classes, counts = np.unique(data_valid, return_counts=True)\n",
    "    total_pixels = counts.sum()\n",
    "    proportions = counts / total_pixels\n",
    "    \n",
    "    # Allocate number of samples per class proportionally\n",
    "    samples_per_class = np.round(proportions * n_esa_sample).astype(int)\n",
    "    \n",
    "    # Adjust to reach exactly n_esa_sample (e.g. 20,000)\n",
    "    diff = n_esa_sample - samples_per_class.sum()\n",
    "    samples_per_class[np.argmax(samples_per_class)] += diff\n",
    "    \n",
    "    # Get row/column indices of valid pixels\n",
    "    rows, cols = np.where(mask if nodata is not None else np.ones_like(data, dtype=bool))\n",
    "\n",
    "    # Stratified random sampling of pixel locations per class\n",
    "    df_list = []\n",
    "    \n",
    "    for cls, n_cls in zip(classes, samples_per_class):\n",
    "        if n_cls == 0:\n",
    "            continue\n",
    "        # Indices of pixels belonging to the current class\n",
    "        idx = np.where(data[rows, cols] == cls)[0]\n",
    "        # Ensure sampling without replacement\n",
    "        if len(idx) < n_cls:\n",
    "            n_cls = len(idx)\n",
    "        \n",
    "        chosen = np.random.choice(idx, size=n_cls, replace=False)\n",
    "        \n",
    "        # Convert pixel indices to geographic coordinates\n",
    "        r = rows[chosen]\n",
    "        c = cols[chosen]\n",
    "        lon, lat = xy(transform, r, c)\n",
    "\n",
    "        # Store sampled points\n",
    "        df_list.append(\n",
    "            pd.DataFrame({\n",
    "                \"esa\": cls,\n",
    "                \"lon\": lon,\n",
    "                \"lat\": lat\n",
    "            })\n",
    "        )\n",
    "    # Create reference dataframe\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    LC_col = \"esa\"\n",
    "    X_col = 'lon'\n",
    "    Y_col = 'lat'\n",
    "    with rasterio.open(embedding_path) as src:\n",
    "        crs = src.crs\n",
    "        if crs.is_epsg_code:\n",
    "            embedding_epsg = crs.to_epsg()\n",
    "            if embedding_epsg != 4326:\n",
    "                # Create transformer from WGS84 to embedding CRS\n",
    "                transformer = Transformer.from_crs(4326, embedding_epsg, always_xy=True)\n",
    "                # Apply the transformation\n",
    "                df[f'x_{embedding_epsg}'], df[f'y_{embedding_epsg}'] = transformer.transform(\n",
    "                    df['lon'].values, df['lat'].values\n",
    "                )\n",
    "                X_col = f'x_{embedding_epsg}'\n",
    "                Y_col = f'y_{embedding_epsg}'\n",
    "            else:\n",
    "                pass     \n",
    "        else:\n",
    "            raise ValueError(\"CRS does not have an EPSG code. Cannot determine embedding EPSG.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92db4c6-7b23-41bb-b1c6-2720db692cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reclassifcation of Land cover class\n",
    "classes_df = pd.read_csv(class_table_path)\n",
    "code_to_value = dict(zip(classes_df[\"code\"], classes_df[\"value\"]))\n",
    "df[\"value\"] = df[LC_col].map(code_to_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4901360-cbf6-4a39-9edb-8ef24c819b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep points that fall within the extent of the embeddings\n",
    "with rasterio.open(embedding_path) as src:\n",
    "    print(\"CRS:\", src.crs)\n",
    "    print(\"Bounds:\", src.bounds)\n",
    "    print(\"Band count:\", src.count)\n",
    "    \n",
    "    xmin = src.bounds.left\n",
    "    xmax = src.bounds.right\n",
    "    ymin = src.bounds.bottom\n",
    "    ymax = src.bounds.top\n",
    "\n",
    "mask = (\n",
    "    (df[X_col] >= xmin) &\n",
    "    (df[X_col] <= xmax) &\n",
    "    (df[Y_col] >= ymin) &\n",
    "    (df[Y_col] <= ymax)\n",
    ")\n",
    "\n",
    "df_in = df[mask].copy()\n",
    "\n",
    "print(\"Points before filtering:\", len(df))\n",
    "print(\"Points after filtering:\", len(df_in))\n",
    "print(f\"Kept: {len(df_in)/len(df):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafaea2-8534-4dac-9991-fb284624e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embedding values at point locations\n",
    "with rasterio.open(embedding_path) as src:\n",
    "    coords = list(zip(df_in[X_col], df_in[Y_col]))\n",
    "    samples = np.array(list(src.sample(coords)))\n",
    "    \n",
    "print(\"Samples shape:\", samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d5280-fb5e-41c4-89e2-a353db9d51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional \n",
    "np.save(os.path.join(output_folder, f\"{sample_filename}.npy\"), samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcad6ce-194e-44a3-a27c-56eb16e2e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (in case values already sampled)\n",
    "samples = np.load(os.path.join(output_folder, f\"{sample_filename}.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58746ee-c3ae-4d34-979c-4eb013561b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame with sampled values\n",
    "band_cols = [f\"band_{i+1}\" for i in range(src.count)]\n",
    "embeddings_df = pd.DataFrame(samples, columns=band_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564785df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataframe for modelling\n",
    "# Safety step: remove rows where all embedding values are 0\n",
    "embeddings_df = embeddings_df[~(embeddings_df == 0).all(axis=1)].reset_index(drop=True)\n",
    "\n",
    "# Concatenate 'value', LC_col, and embeddings into a single dataframe\n",
    "df_final = pd.concat(\n",
    "    [df_in[['value', LC_col]].reset_index(drop=True),\n",
    "     embeddings_df.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Remove rows with NaN values in 'value'\n",
    "df_final = df_final[df_final['value'].notna()].reset_index(drop=True)\n",
    "print(df_final.head())\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aed547-190c-444e-9f3e-cd003734f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save cleaned DataFrame\n",
    "output_file = os.path.join(output_folder, f\"{final_df_filename}.csv\")\n",
    "df_final.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63de6ad-61d7-4cde-b54b-ce7eb64964a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (in case we already have a saved final DataFrame)\n",
    "output_file = os.path.join(output_folder, f\"{final_df_filename}.csv\")\n",
    "df_final = pd.read_csv(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90172a20-5555-4c8c-b78e-44fda28f5a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using ESA data and requested n_sample is larger than available ESA samples,\n",
    "# automatically reduce n_sample to n_esa_sample\n",
    "if is_esa and n_sample > n_esa_sample:\n",
    "    n_sample = n_esa_sample\n",
    "\n",
    "# Class proportions (based on non-aggregated LC classes)\n",
    "class_props = df_final[LC_col].value_counts(normalize=True)\n",
    "\n",
    "# Stratified sampling\n",
    "samples = []\n",
    "\n",
    "for cls, prop in class_props.items():\n",
    "    # Determine number of samples for this class (at least 1)\n",
    "    n_cls = max(1, int(round(prop * n_sample)))\n",
    "    # Randomly sample n_cls rows for the current class\n",
    "    samples.append(\n",
    "        df_final[df_final[LC_col] == cls].sample(n=n_cls, random_state=42)\n",
    "    )\n",
    "    \n",
    "# Concatenate all class-specific samples, shuffle (to prevent potential order bias), and limit total to n_sample\n",
    "df_sample = (\n",
    "    pd.concat(samples)\n",
    "      .sample(n=min(n_sample, sum(len(s) for s in samples)), random_state=42)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for modelling (drop target and original class columns)\n",
    "X = df_sample.drop(columns=['value', LC_col])\n",
    "# Target\n",
    "y = df_sample['value']\n",
    "# Proportion of each class\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove classes with less than 1%\n",
    "to_remove = y.value_counts()[y.value_counts(normalize=True) < 0.01].index\n",
    "for cls in to_remove:\n",
    "    indices = y[y == cls].index\n",
    "    X = X.drop(index=indices)\n",
    "    y = y.drop(index=indices)\n",
    "# Proportion of each class   \n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad004001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sampled data into training and testing sets\n",
    "# 20% test size, stratified by the target class to preserve class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da02546-f077-4949-a31d-e5aa6ca5e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of predictors (used further for model tuning)\n",
    "n_features = X_train.shape[1]\n",
    "print(\"Number of features in the embedding:\", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest classifier\n",
    "# Out-of-bag score disabled (we use cross-validation for hyperparameter tuning)\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=ncore,\n",
    "    oob_score=False,\n",
    "    bootstrap=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b16e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model tuning\n",
    "# Stage 1: Tune individual tree parameters\n",
    "# Define the parameter grid for max_features, max_depth, and min_samples_leaf\n",
    "# This affects tree structure and feature selection per split\n",
    "param_grid_stage1 = {\n",
    "    'max_features': ['sqrt', 'log2', max(1, n_features // 16), max(1, n_features // 8), max(1, n_features // 4)],\n",
    "    'max_depth': [None, 20, 40],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Set up grid search with 5-fold cross-validation and macro F1 score for multiclass\n",
    "grid_stage1 = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid_stage1,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=ncore,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_stage1.fit(X_train, y_train)\n",
    "\n",
    "# Save Stage-1 best parameters to a text file\n",
    "best_stage1_params = grid_stage1.best_params_\n",
    "params_path = os.path.join(output_folder, f\"{best_params_i_filename}.txt\")\n",
    "with open(params_path, \"w\") as f:\n",
    "    f.write(\"Best parameters\\n\")\n",
    "    f.write(\"===================\\n\\n\")\n",
    "    for key, value in best_stage1_params.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# Initialize a new Random Forest with Stage-1 best params for Stage 2\n",
    "rf_stage2_base = RandomForestClassifier(\n",
    "    **best_stage1_params,\n",
    "    random_state=42,\n",
    "    n_jobs=ncore,\n",
    "    bootstrap=True\n",
    ")\n",
    "\n",
    "# Stage 2: Tune forest-level parameters\n",
    "# Parameter grid for n_estimators, min_samples_split, and class_weight\n",
    "# This stage focuses on stability and handling class imbalance\n",
    "param_grid_stage2 = {\n",
    "    'n_estimators': [400, 800, 1200],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "# Set up Stage-2 grid search with 5-fold CV\n",
    "grid_stage2 = GridSearchCV(\n",
    "    estimator=rf_stage2_base,\n",
    "    param_grid=param_grid_stage2,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=ncore,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_stage2.fit(X_train, y_train)\n",
    "\n",
    "# Save Stage-2 best parameters to text file\n",
    "best_params = grid_stage2.best_params_\n",
    "params_path = os.path.join(output_folder, f\"{best_params_ii_filename}.txt\")\n",
    "with open(params_path, \"w\") as f:\n",
    "    f.write(\"Best Hyperparameters\\n\")\n",
    "    f.write(\"===================\\n\\n\")\n",
    "    for key, value in best_params.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "# Retrieve the best Random Forest from Stage 2\n",
    "best_rf = grid_stage2.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "# Save report to text file\n",
    "report_path = os.path.join(output_folder, f\"{report_filename}.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"Classification Report\\n\")\n",
    "    f.write(\"=====================\\n\\n\")\n",
    "    f.write(report)\n",
    "\n",
    "# Get all the parameters\n",
    "best_params = grid_stage2.best_estimator_.get_params()\n",
    "\n",
    "# Initialize the classifier\n",
    "best_rf_final = RandomForestClassifier(\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "# Fit the final model\n",
    "best_rf_final.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45174220-9adf-4623-ad20-e761d815ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save final model\n",
    "model_path = os.path.join(output_folder, f\"{best_model_filename}.pkl\")\n",
    "joblib.dump(best_rf_final, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2e5d0-e72f-409d-95ea-74c5fb95163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (in case we have already calibrated and saved the model)\n",
    "model_path = os.path.join(output_folder, f\"{best_model_filename}.pkl\")\n",
    "best_rf_final = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339225c-95c8-4ced-9da2-09edb66039c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read embedding and check if the number of features is correct\n",
    "src = rasterio.open(embedding_path)\n",
    "print(\"Number of bands in raster:\", src.count)\n",
    "print(\"Number of features in model:\", best_rf_final.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669e51b-b9e7-4c30-8d64-3bd872bfb0ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process raster in smaller blocks to save memory\n",
    "block_shape = (512, 512)  # smaller block to be safe\n",
    "predicted_path = os.path.join(output_folder, f\"{prediction_filename}.tif\")\n",
    "\n",
    "# Open input raster\n",
    "with rasterio.open(embedding_path) as src:\n",
    "    profile = src.profile\n",
    "    n_bands = src.count\n",
    "    n_rows, n_cols = src.height, src.width\n",
    "\n",
    "    # Prepare output raster profile: single band, uint8\n",
    "    profile.update(count=1, dtype=rasterio.uint8)\n",
    "\n",
    "    # Loop over raster in blocks to avoid loading entire raster into memory\n",
    "    with rasterio.open(predicted_path, 'w', **profile) as dst_pred:\n",
    "        for i in range(0, n_rows, block_shape[0]):\n",
    "            for j in range(0, n_cols, block_shape[1]):\n",
    "                try:\n",
    "                    # Memory check (if needed, import psutil)\n",
    "                    # mem = psutil.virtual_memory()\n",
    "                    # print(f\"Processing block at row {i}, col {j} | Available RAM: {mem.available/1e6:.1f} MB\")\n",
    "\n",
    "                    # Define window (subset of raster)\n",
    "                    window = rasterio.windows.Window(\n",
    "                        j, i,\n",
    "                        min(block_shape[1], n_cols-j),\n",
    "                        min(block_shape[0], n_rows-i)\n",
    "                    )\n",
    "\n",
    "                    # Read the block (n_bands, height, width)\n",
    "                    block = src.read(window=window)  # shape: (n_bands, h, w)\n",
    "                    n_b, h, w = block.shape\n",
    "\n",
    "                     # Flatten block for prediction (pixels x features)\n",
    "                    X_block = block.reshape(n_b, -1).T\n",
    "                    # Convert to float32 to save memory\n",
    "                    X_block = X_block.astype(np.float32)\n",
    "\n",
    "                    # Optional: avoid DataFrame if memory is tight\n",
    "                    # X_block = pd.DataFrame(X_block, columns=X.columns)\n",
    "\n",
    "                     # Predict classes for all pixels in block\n",
    "                    y_block_pred = best_rf_final.predict(X_block)\n",
    "\n",
    "                    # Reshape predictions back to raster block shape\n",
    "                    y_block_pred_raster = y_block_pred.reshape(h, w)\n",
    "\n",
    "                     # Write predicted block to output raster\n",
    "                    dst_pred.write(y_block_pred_raster, 1, window=window)\n",
    "\n",
    "                    print(f\"Block at row {i}, col {j} processed successfully\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR at block row {i}, col {j}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tessera)",
   "language": "python",
   "name": "tessera"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
